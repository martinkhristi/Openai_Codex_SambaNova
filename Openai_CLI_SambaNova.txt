
OpenAI Codex CLI with SambaNova üöÄ

This repository demonstrates how to install and configure the OpenAI Codex CLI to run with SambaNova-hosted models.

With just a few steps, you‚Äôll be able to connect the Codex CLI to SambaNova‚Äôs API and run powerful open-source LLMs like Llama-4 Maverick directly from your terminal.
.

üì¶ Installation

Install Codex CLI globally with your preferred package manager:

npm install -g @openai/codex

Or with Homebrew:
brew install codex

‚öôÔ∏è Step 2: Create Your Config File

Codex stores its configuration in:

Windows: %USERPROFILE%\.codex\config.toml

macOS/Linux: $HOME/.codex/config.toml

Open it in your editor, e.g., Notepad on Windows:

notepad %USERPROFILE%\.codex\config.toml

Paste the following configuration:

disable_response_storage = true
show_reasoning_content   = true

[model_providers.sambanova]
name     = "sambanova"
base_url = "https://api.sambanova.ai/v1"
env_key  = "SAMBANOVA_API_KEY"

[profiles.oss]
model_provider = "sambanova"
model = "Llama-4-Maverick-17B-128E-Instruct"


üîë Step 3: Set Your API Key

Set your SambaNova API key as an environment variable:
get your free API key at https://sambanova.ai/

setx SAMBANOVA_API_KEY "YOUR_KEY_HERE"   # Windows  
export SAMBANOVA_API_KEY="YOUR_KEY_HERE" # macOS/Linux


‚ñ∂Ô∏è Step 4: Run Codex

Now, run Codex with your new profile:
codex --profile oss

Check the connection status:
/status

you should see:
Provider: Sambanova ‚úÖ

‚úÖ You‚Äôre Ready!

You can now start using Codex CLI with SambaNova to:

Run inference directly from your terminal

Access enterprise-grade Llama models

Automate workflows with open-source AI













