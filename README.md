OpenAI Codex CLI with SambaNova

Install Codex CLI globally using npm or brew

Create the config file in %USERPROFILE%\.codex\config.toml

Add settings: disable response storage, show reasoning content, and set SambaNova as the provider with the Llama-4 Maverick model

Set your SAMBANOVA_API_KEY as an environment variable

Run Codex with the profile oss

Check status — you should see “Provider: Sambanova ✅”
